# 冻结训练

> 冻结训练：当我们已有部分预训练权重，这部分预训练权重所应用的部分网络是通用的（如骨干网络）。那么可以先冻结骨干网络的权重训练，将更多的资源放在训练后面的部分。等到后面部分训练差不多时，再将二者解冻，同时训练。

## 冻结方法

#### 方法1

**冻结**

```python
model.layer1.weight.requires_grad = False
optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.1)
```

**解冻**

```python
model.layer1.weight.requires_grad = True
optimizer.add_param_group({'params': model.layer1.parameters()})
```

#### 方法2

**冻结**

```python
optimizer = optim.Adam([{'params':[ param for name, param in model.named_parameters() if 'fc1' not in name]}], lr=0.1)
# optim.Adam(params, lr)
# 其中 params是一个迭代器（字典），包含需要迭代更新的参数。
```

**解冻**

```python
optimizer.add_param_group({'params': model.fc1.parameters()})
```

#### 方法3

在反向传播之前，将原来的layer的权重存储，反向传播后，再将存储的权重写回。

```python
fc1_old_weights = Variable(model.fc1.weight.data.clone())
# compute loss
# loss.backward()
# optimizer.step()
model.fc1.weight.data = fc1_old_weights.data
```